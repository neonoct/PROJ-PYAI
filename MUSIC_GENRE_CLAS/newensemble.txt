Best parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}
Best cross-validation accuracy for Decision Tree: 0.519325
              precision    recall  f1-score   support

           0       0.41      0.30      0.35      1008
           1       0.71      0.70      0.71      1034
           2       0.51      0.49      0.50      1021
           3       0.78      0.77      0.77       955
           4       0.48      0.50      0.49       986
           5       0.57      0.52      0.54      1009
           6       0.43      0.42      0.43       995
           7       0.48      0.39      0.43       985
           8       0.43      0.45      0.44      1030
           9       0.47      0.74      0.58       977

    accuracy                           0.53     10000
   macro avg       0.53      0.53      0.52     10000
weighted avg       0.53      0.53      0.52     10000

[[305  24  26   2 174  49  68  54  74 232]
 [ 15 727  98  69  35  59   0  22   1   8]
 [ 62  82 502  37  89  78   5 105   4  57]
 [ 21  72  30 731  10  30   0  56   0   5]
 [ 84  24  51   5 495  25  23  44  23 212]
 [ 70  60 128  13  43 523  19 104  16  33]
 [ 37   2   4   0  17  17 417  13 423  65]
 [ 62  23 133  74  98 123  20 388  11  53]
 [ 27   0   6   0  10   4 376   2 463 142]
 [ 55   4  10   4  53   9  34  18  64 726]]
Best parameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}
Best cross-validation accuracy for Random Forest: 0.568525
              precision    recall  f1-score   support

           0       0.51      0.28      0.36      1008
           1       0.78      0.71      0.74      1034
           2       0.61      0.50      0.55      1021
           3       0.82      0.84      0.83       955
           4       0.50      0.58      0.54       986
           5       0.63      0.61      0.62      1009
           6       0.42      0.48      0.45       995
           7       0.53      0.50      0.52       985
           8       0.41      0.35      0.38      1030
           9       0.48      0.80      0.60       977

    accuracy                           0.56     10000
   macro avg       0.57      0.57      0.56     10000
weighted avg       0.57      0.56      0.56     10000

[[284   9  17   0 196  66  85  74  48 229]
 [ 22 733  60  86  57  53   0  15   0   8]
 [ 46  81 507  21 112  62   4 125   2  61]
 [ 26  31  25 805  11  17   0  38   0   2]
 [ 32  22  32   0 571  15  27  47  13 227]
 [ 44  45  80   5  52 618  17 105   9  34]
 [ 14   0   2   0  14  11 479   8 390  77]
 [ 20  13  95  58  87 138  26 492   6  50]
 [ 16   0   2   0  10   2 468   3 359 170]
 [ 52   6   5   2  30   3  32  17  47 783]]

 Best parameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.7}
Best cross-validation accuracy for XGBoost: 0.5897
              precision    recall  f1-score   support

           0       0.48      0.37      0.42      1008
           1       0.82      0.76      0.79      1034
           2       0.63      0.55      0.59      1021
           3       0.86      0.84      0.85       955
           4       0.56      0.60      0.58       986
           5       0.69      0.63      0.66      1009
           6       0.42      0.41      0.41       995
           7       0.54      0.54      0.54       985
           8       0.42      0.44      0.43      1030
           9       0.52      0.74      0.61       977

    accuracy                           0.59     10000
   macro avg       0.59      0.59      0.59     10000
weighted avg       0.59      0.59      0.59     10000

[[369  13  12   1 158  47  74  74  63 197]
 [ 25 790  60  54  39  39   0  19   0   8]
 [ 56  66 563  20  94  38   5 124   1  54]
 [ 25  40  24 804   4   9   0  46   0   3]
 [ 64  10  37   0 594  13  26  53  16 173]
 [ 54  35  79   5  35 637  19 108  16  21]
 [ 30   0   1   0   7  15 409  10 474  49]
 [ 45   9 107  54  54 115  15 536   5  45]
 [ 30   0   1   0  12   3 407   7 450 120]
 [ 71   6   5   1  61   7  30  20  50 726]]

               precision    recall  f1-score   support

           0       0.49      0.34      0.40      1008
           1       0.79      0.74      0.76      1034
           2       0.62      0.53      0.57      1021
           3       0.83      0.82      0.83       955
           4       0.54      0.58      0.56       986
           5       0.67      0.61      0.64      1009
           6       0.41      0.43      0.42       995
           7       0.54      0.52      0.53       985
           8       0.41      0.40      0.40      1030
           9       0.49      0.77      0.60       977

    accuracy                           0.57     10000
   macro avg       0.58      0.57      0.57     10000
weighted avg       0.58      0.57      0.57     10000

[[344  14  12   1 164  46  82  63  59 223]
 [ 18 760  70  65  48  47   0  17   0   9]
 [ 55  75 542  24 106  43   5 116   2  53]
 [ 26  46  23 787   6  18   0  46   0   3]
 [ 52  11  32   1 572  11  26  55  17 209]
 [ 59  36  85   7  40 618  17 106  13  28]
 [ 23   0   2   0   9  15 432   9 438  67]
 [ 39  14  98  63  68 112  24 515   6  46]
 [ 24   0   3   0   4   4 439   5 407 144]
 [ 63   6   3   2  42   8  33  20  47 753]]
Test accuracy of Voting Classifier: 0.573